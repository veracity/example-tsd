{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ship_list(ship):\n",
    "    ship = [word for word in ship.lower().split(\" \") if len(word) > 2 or word.isdigit()]\n",
    "    return ship\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tweet(tweet):\n",
    "    return [word for word in tweet if not (word in stop_words or word.startswith(\"https://\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = [line.rstrip() for line in open(\"../data_files/stop_words.txt\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "tweets = open(\"../data_files/tweets_with_location.csv\", \"r\", encoding=\"utf-8\")\n",
    "tweets = [tweet for tweet in csv.DictReader(tweets, delimiter=\"\\t\")]\n",
    "texts = [tweet[\"tweet\"] for tweet in tweets]\n",
    "formatted_tweets = [format_tweet(tweet.replace(\"#\",\"\").lower().split()) for tweet in texts]\n",
    "\n",
    "ships = open(\"../data_files/ships.csv\", \"r\", encoding=\"utf-8\")\n",
    "ships = [(ship[\"ship_name\"]) for ship in csv.DictReader(ships)]\n",
    "formatted_ships = [generate_ship_list(ship) for ship in ships]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scores(ship):\n",
    "    return [0.6] + [0.4/len(ship[1:]) for i in ship[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ship_name(tweet, ships, formatted_ships):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for index, ship in enumerate(formatted_ships):\n",
    "        score = 0\n",
    "        scores = generate_scores(ship)\n",
    "        for i, word in enumerate(tweet):\n",
    "            if word.startswith(\"https://\"): continue\n",
    "            for term in ship:\n",
    "                if term.isdigit():                    \n",
    "                    if re.search(f\"^\\D*{term}\\D*$\", word):\n",
    "                        score += 0.6\n",
    "                        term = False\n",
    "                else:\n",
    "                    if term in word: \n",
    "                        score += 0.2\n",
    "                        term = False\n",
    "            if score == 1:\n",
    "                \n",
    "                return ships[index], score\n",
    "            if score > best_score: \n",
    "                best_score = score\n",
    "                best_match = index\n",
    "\n",
    "    return ships[best_match] if best_match else None, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_with_ship(word, ship_word):\n",
    "    if re.search(f\"\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ship_name2(tweet, ships, formatted_ships):\n",
    "    best_matches = []\n",
    "    best_score = 0\n",
    "    tweet = \" \".join(tweet)\n",
    "    index = 0\n",
    "\n",
    "    print(tweet)\n",
    "    for ship in formatted_ships:\n",
    "        number_matching = bool(re.search(f\"\\D*{ship[0]}\\D*\", tweet))\n",
    "        matching_terms = [number_matching]\n",
    "        for term in ship[1:]:\n",
    "            matching_terms.append(True if term in tweet else False)\n",
    "        if matching_terms[0] and not True in matching_terms[1:]:\n",
    "            index += 1\n",
    "            continue\n",
    "        else: \n",
    "            if matching_terms.count(True) == best_score:\n",
    "                best_matches.append(index)\n",
    "            elif matching_terms.count(True) > best_score:\n",
    "                best_score = matching_terms.count(True)\n",
    "                best_matches = [index]\n",
    "        \n",
    "        index += 1\n",
    "    return [ships[i] for i in best_matches], best_score\n",
    "                                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(ship_name, tweet, locations):\n",
    "    for word in tweet: \n",
    "        if word in locations and not word in ship_name:\n",
    "            return word\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets_to_file(tweets, ships):\n",
    "    formatted_ships = [generate_ship_list(ship) for ship in ships]\n",
    "    formatted_tweets = [format_tweet(tweet.replace(\"#\",\"\").lower().split()) for tweet in texts]\n",
    "    ship_history = {}\n",
    "    for i in range(len(formatted_tweets)):\n",
    "        ship, prob = extract_ship_name(formatted_tweets[i], ships, formatted_ships)\n",
    "        \n",
    "        if ship:\n",
    "            ship = ship.replace(\" \", \"_\")\n",
    "            if ship in ship_history:\n",
    "                ship_history[ship].append({\n",
    "                    \"created_at\": tweets[i][\"created_at\"],\n",
    "                    \"tweet\": tweets[i][\"tweet\"],\n",
    "                    \"location\": tweets[i][\"location\"],\n",
    "                    \"id\": tweets[i][\"id\"].replace(\" \", \"\"),\n",
    "                    \"id_str\": tweets[i][\"id_str\"].replace(\" \", \"\")\n",
    "                })\n",
    "            else:\n",
    "                ship_history[ship] = [{\n",
    "                    \"created_at\": tweets[i][\"created_at\"],\n",
    "                    \"tweet\": tweets[i][\"tweet\"],\n",
    "                    \"location\": tweets[i][\"location\"],\n",
    "                    \"id\": tweets[i][\"id\"].replace(\" \", \"\"),\n",
    "                    \"id_str\": tweets[i][\"id_str\"].replace(\" \", \"\")\n",
    "                }]\n",
    "    print(\"Writing files\")    \n",
    "    for key in ship_history.keys():\n",
    "        ship_metadata = data[key]\n",
    "        ship_metadata[\"name\"] = key.replace(\"_\", \" \")\n",
    "        with open(f\"../ships2/{key}.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            ship = {\n",
    "                \"metadata\": ship_metadata,\n",
    "                \"history\": ship_history[key]\n",
    "            }\n",
    "            file.write(json.dumps(ship))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_tweets_to_file(tweets, ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_all_tweets(tweets, ships):\n",
    "    formatted_ships = [generate_ship_list(ship) for ship in ships]\n",
    "    formatted_tweets = [format_tweet(tweet.replace(\"#\",\"\").lower().split()) for tweet in texts]\n",
    "    all_tweets = []\n",
    "    for i in range(len(formatted_tweets)):\n",
    "        ship, prob = extract_ship_name(formatted_tweets[i], ships, formatted_ships)\n",
    "        if ship:\n",
    "            ship = ship.replace(\" \",\"_\")\n",
    "        \n",
    "        all_tweets.append({\n",
    "            \"created_at\": tweets[i][\"created_at\"],\n",
    "            \"tweet\": tweets[i][\"tweet\"],\n",
    "            \"location\": tweets[i][\"location\"],\n",
    "            \"id\": tweets[i][\"id\"].replace(\" \", \"\"),\n",
    "            \"id_str\": tweets[i][\"id_str\"].replace(\" \", \"\"),\n",
    "            \"ship\": ship\n",
    "        })\n",
    "    \n",
    "    with open(\"tweets_with_ships.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(json.dumps(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_all_tweets(tweets, ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ships = open(\"../data_files/ships3.json\", \"r\", encoding=\"utf-8\")\n",
    "data = json.load(new_ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"RS_3_Tordenskjold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [False, True, False]\n",
    "print(x.count(False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
